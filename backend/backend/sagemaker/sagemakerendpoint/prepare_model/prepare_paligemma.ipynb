{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PaliGemma for deployment\n",
    "\n",
    "Run the cells below and follow the instructions to deploy the model to the endpoint. You should set the PROCESSING_DIR variable to a directory on your machine which is not git-tracked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSING_DIR = \"./TEMPS\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {PROCESSING_DIR}/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./TEMPS/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROCESSING_DIR}/code/requirements.txt\n",
    "accelerate\n",
    "bitsandbytes\n",
    "git+https://github.com/huggingface/transformers.git@v4.41.2\n",
    "Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./TEMPS/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROCESSING_DIR}/code/inference.py\n",
    "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_image(img_bytes):\n",
    "    buffer = BytesIO(img_bytes)\n",
    "    img = Image.open(buffer)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "def initialize_model(model_path):\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_path, \n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "    processor = PaliGemmaProcessor.from_pretrained(model_path)\n",
    "    return model, processor\n",
    "\n",
    "def generate_response(input_data, model_and_processor):\n",
    "    model, processor = model_and_processor\n",
    "    \n",
    "    text_prompt = input_data.get(\"prompt\", \"\")\n",
    "    image_data = base64.b64decode(input_data.get(\"image\", \"\"))\n",
    "    \n",
    "    processed_image = process_image(image_data)\n",
    "\n",
    "    model_input = processor(\n",
    "        text=text_prompt, \n",
    "        images=processed_image, \n",
    "        padding=\"longest\", \n",
    "        do_convert_rgb=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device, dtype=model.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**model_input, max_length=496)\n",
    "        output_text = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\"response\": output_text}\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return initialize_model(model_dir)\n",
    "\n",
    "def predict_fn(data, model_process):\n",
    "    return generate_response(data, model_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the model\n",
    "\n",
    "Alternatively to the following code, you can download the model by yourself (using Huggingface CLI, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e087f642084fcba622bc030a15e40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79e617624864c86b7a3cd9685de6a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47e25c04b2c4c2997b5c724c6a9e0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78315b4b1e344686ae2daf37d311f165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa6649534204137b09e672e657cbcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8329ba1f4a984624842f606e2684f30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4bfed4750f44bd9a9d9b9b9ebf7bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa94afd899f4f1d903f0cbbb012c1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5643ee59a54841f4ad5333bbb83c55d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d612ba15df346f8be5d5f1007c96c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8701629e8b4556b8cfe6443f0023f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c430b3b2c04416eaac4f3ab1de35902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abbe35f2fbc40d89585da4c15fc3179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a09635ace1494fba5ccc87590ddd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7f285c213a48beb1a6094c9d6b1006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('TEMPS/model-9804/code')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from shutil import copytree\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_IDENTIFIER = \"google/paligemma-3b-mix-224\"\n",
    "\n",
    "# Prerequisite: Accept the Gemma terms and conditions: https://huggingface.co/google/paligemma-3b-mix-224\n",
    "# Enter your HuggingFace token: https://huggingface.co/settings/tokens\n",
    "user_token = input(\"Enter your HuggingFace token: \")\n",
    "if not user_token:\n",
    "    raise ValueError(\"HuggingFace token is required.\")\n",
    "\n",
    "# Fetch model snapshot\n",
    "snapshot_path = snapshot_download(\n",
    "    repo_id=MODEL_IDENTIFIER,\n",
    "    use_auth_token=user_token,\n",
    "    local_dir=Path(PROCESSING_DIR, \"hf_download\")\n",
    ")\n",
    "\n",
    "# Create model directory with random name\n",
    "unique_id = random.getrandbits(16)\n",
    "model_directory = Path(PROCESSING_DIR, f\"model-{unique_id}\")\n",
    "model_directory.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy snapshot to model directory\n",
    "copytree(snapshot_path, str(model_directory), dirs_exist_ok=True)\n",
    "\n",
    "# Copy code/ to model directory\n",
    "copytree(Path(PROCESSING_DIR, \"code\"), model_directory.joinpath(\"code\"), dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./.cache/\n",
      "./.cache/huggingface/\n",
      "./.cache/huggingface/.gitignore\n",
      "./.cache/huggingface/download/\n",
      "./.cache/huggingface/download/model-00001-of-00003.safetensors.lock\n",
      "./.cache/huggingface/download/model-00002-of-00003.safetensors.lock\n",
      "./.cache/huggingface/download/generation_config.json.lock\n",
      "./.cache/huggingface/download/.gitattributes.lock\n",
      "./.cache/huggingface/download/README.md.lock\n",
      "./.cache/huggingface/download/config.json.lock\n",
      "./.cache/huggingface/download/model-00003-of-00003.safetensors.lock\n",
      "./.cache/huggingface/download/added_tokens.json.lock\n",
      "./.cache/huggingface/download/config.json.metadata\n",
      "./.cache/huggingface/download/model.safetensors.index.json.lock\n",
      "./.cache/huggingface/download/generation_config.json.metadata\n",
      "./.cache/huggingface/download/README.md.metadata\n",
      "./.cache/huggingface/download/preprocessor_config.json.lock\n",
      "./.cache/huggingface/download/special_tokens_map.json.lock\n",
      "./.cache/huggingface/download/added_tokens.json.metadata\n",
      "./.cache/huggingface/download/tokenizer.json.lock\n",
      "./.cache/huggingface/download/model.safetensors.index.json.metadata\n",
      "./.cache/huggingface/download/tokenizer.model.lock\n",
      "./.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "./.cache/huggingface/download/tokenizer_config.json.lock\n",
      "./.cache/huggingface/download/preprocessor_config.json.metadata\n",
      "./.cache/huggingface/download/tokenizer.json.metadata\n",
      "./.cache/huggingface/download/tokenizer.model.metadata\n",
      "./.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "./.cache/huggingface/download/.gitattributes.metadata\n",
      "./.cache/huggingface/download/model-00003-of-00003.safetensors.metadata\n",
      "./.cache/huggingface/download/model-00001-of-00003.safetensors.metadata\n",
      "./.cache/huggingface/download/model-00002-of-00003.safetensors.metadata\n",
      "./config.json\n",
      "./generation_config.json\n",
      "./README.md\n",
      "./added_tokens.json\n",
      "./model.safetensors.index.json\n",
      "./special_tokens_map.json\n",
      "./preprocessor_config.json\n",
      "./tokenizer.json\n",
      "./tokenizer.model\n",
      "./tokenizer_config.json\n",
      "./.gitattributes\n",
      "./model-00003-of-00003.safetensors\n",
      "./model-00001-of-00003.safetensors\n",
      "./model-00002-of-00003.safetensors\n",
      "./code/\n",
      "./code/requirements.txt\n",
      "./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf {PROCESSING_DIR}/model.tar.gz -C {model_directory} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# Upload model.tar.gz to s3\n",
    "account_id=input(\"Please fill in your AWS account id: \")\n",
    "s3_model_uri=S3Uploader.upload(local_path=f\"{PROCESSING_DIR}/model.tar.gz\", desired_s3_uri=f\"s3://vis-assis-sagemaker-endpoint-model-{account_id}/paligemma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
