{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare PaliGemma for deployment\n",
    "\n",
    "Run the cells below and follow the instructions to deploy the model to the endpoint. You should set the PROCESSING_DIR variable to a directory on your machine which is not git-tracked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSING_DIR = \"./TEMPS\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/tmp/sample-genai-edge-assistant/backend/backend/sagemaker/sagemakerendpoint/prepare_model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {PROCESSING_DIR}/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./TEMPS/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROCESSING_DIR}/code/requirements.txt\n",
    "accelerate\n",
    "bitsandbytes\n",
    "git+https://github.com/huggingface/transformers.git@v4.41.2\n",
    "Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./TEMPS/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROCESSING_DIR}/code/inference.py\n",
    "from transformers import AutoTokenizer, PaliGemmaForConditionalGeneration, PaliGemmaProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_image(img_bytes):\n",
    "    buffer = BytesIO(img_bytes)\n",
    "    img = Image.open(buffer)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "def initialize_model(model_path):\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_path, \n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device,\n",
    "        revision=\"bfloat16\",\n",
    "    ).eval()\n",
    "    processor = PaliGemmaProcessor.from_pretrained(model_path)\n",
    "    return model, processor\n",
    "\n",
    "def generate_response(input_data, model_and_processor):\n",
    "    model, processor = model_and_processor\n",
    "    \n",
    "    text_prompt = input_data.get(\"prompt\", \"\")\n",
    "    image_data = base64.b64decode(input_data.get(\"image\", \"\"))\n",
    "    \n",
    "    processed_image = process_image(image_data)\n",
    "\n",
    "    model_input = processor(\n",
    "        text=text_prompt, \n",
    "        images=processed_image, \n",
    "        padding=\"longest\", \n",
    "        do_convert_rgb=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device, dtype=model.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**model_input, max_length=496)\n",
    "        output_text = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\"response\": output_text}\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return initialize_model(model_dir)\n",
    "\n",
    "def predict_fn(data, model_process):\n",
    "    return generate_response(data, model_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the model\n",
    "\n",
    "Alternatively to the following code, you can download the model by yourself (using Huggingface CLI, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffba568caa854543be223ed1731f88ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('TEMPS/model-47549/code')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from shutil import copytree\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_IDENTIFIER = \"google/paligemma-3b-mix-224\"\n",
    "\n",
    "# Prerequisite: Accept the Gemma terms and conditions: https://huggingface.co/google/paligemma-3b-mix-224\n",
    "# Enter your HuggingFace token: https://huggingface.co/settings/tokens\n",
    "user_token = input(\"Enter your HuggingFace token: \")\n",
    "if not user_token:\n",
    "    raise ValueError(\"HuggingFace token is required.\")\n",
    "\n",
    "# Fetch model snapshot\n",
    "snapshot_path = snapshot_download(\n",
    "    repo_id=MODEL_IDENTIFIER,\n",
    "    use_auth_token=user_token,\n",
    "    local_dir=Path(PROCESSING_DIR, \"hf_download\")\n",
    ")\n",
    "\n",
    "# Create model directory with random name\n",
    "unique_id = random.getrandbits(16)\n",
    "model_directory = Path(PROCESSING_DIR, f\"model-{unique_id}\")\n",
    "model_directory.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy snapshot to model directory\n",
    "copytree(snapshot_path, str(model_directory), dirs_exist_ok=True)\n",
    "\n",
    "# Copy code/ to model directory\n",
    "copytree(Path(PROCESSING_DIR, \"code\"), model_directory.joinpath(\"code\"), dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./.cache/\n",
      "./.cache/huggingface/\n",
      "./.cache/huggingface/.gitignore\n",
      "./.cache/huggingface/download/\n",
      "./.cache/huggingface/download/added_tokens.json.lock\n",
      "./.cache/huggingface/download/model-00001-of-00003.safetensors.lock\n",
      "./.cache/huggingface/download/model-00002-of-00003.safetensors.lock\n",
      "./.cache/huggingface/download/model-00003-of-00003.safetensors.lock\n",
      "./.cache/huggingface/download/config.json.lock\n",
      "./.cache/huggingface/download/.gitattributes.lock\n",
      "./.cache/huggingface/download/generation_config.json.lock\n",
      "./.cache/huggingface/download/README.md.lock\n",
      "./.cache/huggingface/download/added_tokens.json.metadata\n",
      "./.cache/huggingface/download/model.safetensors.index.json.lock\n",
      "./.cache/huggingface/download/config.json.metadata\n",
      "./.cache/huggingface/download/preprocessor_config.json.lock\n",
      "./.cache/huggingface/download/README.md.metadata\n",
      "./.cache/huggingface/download/.gitattributes.metadata\n",
      "./.cache/huggingface/download/special_tokens_map.json.lock\n",
      "./.cache/huggingface/download/tokenizer.json.lock\n",
      "./.cache/huggingface/download/model.safetensors.index.json.metadata\n",
      "./.cache/huggingface/download/tokenizer.model.lock\n",
      "./.cache/huggingface/download/generation_config.json.metadata\n",
      "./.cache/huggingface/download/tokenizer_config.json.lock\n",
      "./.cache/huggingface/download/preprocessor_config.json.metadata\n",
      "./.cache/huggingface/download/special_tokens_map.json.metadata\n",
      "./.cache/huggingface/download/tokenizer_config.json.metadata\n",
      "./.cache/huggingface/download/tokenizer.model.metadata\n",
      "./.cache/huggingface/download/tokenizer.json.metadata\n",
      "./.cache/huggingface/download/model-00003-of-00003.safetensors.metadata\n",
      "./.cache/huggingface/download/model-00002-of-00003.safetensors.metadata\n",
      "./.cache/huggingface/download/model-00001-of-00003.safetensors.metadata\n",
      "./added_tokens.json\n",
      "./config.json\n",
      "./README.md\n",
      "./.gitattributes\n",
      "./model.safetensors.index.json\n",
      "./generation_config.json\n",
      "./preprocessor_config.json\n",
      "./special_tokens_map.json\n",
      "./tokenizer_config.json\n",
      "./tokenizer.model\n",
      "./tokenizer.json\n",
      "./model-00003-of-00003.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model-00002-of-00003.safetensors\n",
      "./model-00001-of-00003.safetensors\n",
      "./code/\n",
      "./code/requirements.txt\n",
      "./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf {PROCESSING_DIR}/model.tar.gz -C {model_directory} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# Upload model.tar.gz to s3\n",
    "account_id=\"\" #input(\"Please fill in your AWS account id: \")\n",
    "s3_model_uri=S3Uploader.upload(local_path=f\"{PROCESSING_DIR}/model.tar.gz\", desired_s3_uri=f\"s3://vis-assis-sagemaker-endpoint-model-{account_id}/paligemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
